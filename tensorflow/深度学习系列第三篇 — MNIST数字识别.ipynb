{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习系列第三篇 — MNIST数字识别\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一节将上一节学到的深度神经网络的概念运用起来，通过 tf 来实现 MNIST 手写字识别。\n",
    "首先导入 tf 库和训练数据：\n",
    "```\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "```\n",
    "\n",
    "定义全局初始常量，其中 INPUT_NODE 数为每一张图片 28 * 28 的像素数，OUTPUT_NODE 就是分类的个数 10； LAYER1_NODE 为隐藏层节点数，BATCH_SIZE 为每次训练数据的个数；LEARNING_RATE_BASE 为基础学习率，LEARNING_RATE_DECAY 为学习率的衰减率，REGULARIZATION_RATE 为正则化损失函数的系数，TRAINING_STEPS 为训练的次数，MOVING_AVERAGE_DECAY 为滑动平均衰减率。\n",
    "```\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "```\n",
    "\n",
    "定义一个 inference 函数用来计算神经网络的向前传播结果，并且通过 RELU 函数实现了去线性化。avg_class 参数是用来支持测试时使用滑动平均模型，当我们使用了滑动平均模型时，weights 和 biases 值都是从 avg_class 中取出的。\n",
    "\n",
    "```\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    if avg_class is None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "```\n",
    "\n",
    "定义输入层，生成隐藏层和输出层参数\n",
    "\n",
    "```\n",
    "x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "\n",
    "weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "```\n",
    "\n",
    "计算当前参数下神经网络向前传播的效果。\n",
    "```\n",
    "y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "```\n",
    "\n",
    "这里通过滑动平均衰减率和训练次数初始化这个类，用来加快训练早期变量的更新速度；global_step 为动态存储训练次数。\n",
    "\n",
    "```\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "```\n",
    "\n",
    "variables_averages_op 这里将所有的神经网络的上参数使用滑动平均，对于指定 trainable=False 的参数不作用。计算使用了滑动平均模型处理的向前传播结果。\n",
    "```\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "```\n",
    "\n",
    "计算损失。交叉熵用来刻画预测值与真实值差距的损失函数，我们再通过 softmax 回归将结果变成概率分布。tf 提供了将这两个函数合并使用的函数，第一个参数是向前传播的结果，第二个参数是训练数据的答案。然后计算所有样例的交叉熵平均值。\n",
    "\n",
    "```\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "```\n",
    "\n",
    "这里使用 L2 正则化损失函数，计算模型的正则化损失，计算权重的，不计算偏置。正则化损失函数用来避免过拟合。\n",
    "\n",
    "```\n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "regularization = regularizer(weights1) + regularizer(weights2)\n",
    "```\n",
    "\n",
    "最后得出的总损失等于交叉熵损失和正则化损失之和。\n",
    "```\n",
    "loss = cross_entropy_mean + regularization\n",
    "```\n",
    "\n",
    "设置指数衰减的学习率。\n",
    "\n",
    "```\n",
    "learnging_rate = tf.train.exponential_decay(\n",
    "    LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "```\n",
    "\n",
    "使用优化算法优化总损失。\n",
    "\n",
    "```\n",
    "train_step = tf.train.GradientDescentOptimizer(learnging_rate).minimize(loss, global_step=global_step)\n",
    "```\n",
    "\n",
    "每过一次数据需要更新一下参数。\n",
    "```\n",
    "with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    train_op = tf.no_op()\n",
    "```\n",
    "\n",
    "检验使用了滑动平均模型的向前传播结果是否正确。\n",
    "```\n",
    "correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "```\n",
    "\n",
    "计算平均准确率。\n",
    "```\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "```\n",
    "\n",
    "最后开始我们的训练，并验证数据的准确率。\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    # 初始化全部变量\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 准备验证数据\n",
    "    validate_feed = {x: mnist.validation.images,\n",
    "                    y_: mnist.validation.labels}\n",
    "    # 准备测试数据\n",
    "    test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "    # 迭代\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        if i % 1000 == 0:\n",
    "            # 使用全部的验证数据去做了验证\n",
    "            validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "            print \"训练轮数：\", i, \"，准确率：\", validate_acc * 100, \"%\"\n",
    "        # 取出一部分训练数据\n",
    "        xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "        # 训练\n",
    "        sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "    # 计算最终的准确率。\n",
    "    test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "    print \"训练轮数：\", TRAINING_STEPS, \"，准确率：\", test_acc * 100, \"%\"\n",
    "```\n",
    "\n",
    "开始训练的过程，首先初始化所有变量。\n",
    "```\n",
    "tf.global_variables_initializer().run()\n",
    "```\n",
    "\n",
    "MNIST 数据分为训练数据、验证数据和测试数据。我们先准备好验证数据和测试数据，因为数据量不大，可以直接将全部数据用于训练。然后开始我们的迭代训练，训练数据有很多，我们每次训练只取一部分数据进行训练，这样减小计算量，加速神经网络的训练，又不会对结果产生太大影响。\n",
    "\n",
    "tf 的训练通过  sess.run 函数，第一个参数是最终要计算的，也就是公式的输出，第二个参数 feed 是 placeholder 的输入。\n",
    "```\n",
    "sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "```\n",
    "\n",
    "通过一次次的训练，总损失会越来越小，模型的预测越来越准确，到达一个临界点。\n",
    "\n",
    "完整代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "训练轮数： 0 ，准确率： 9.20000001788 %\n",
      "训练轮数： 1000 ，准确率： 97.619998455 %\n",
      "训练轮数： 2000 ，准确率： 98.0799973011 %\n",
      "训练轮数： 3000 ，准确率： 98.2599973679 %\n",
      "训练轮数： 4000 ，准确率： 98.1999993324 %\n",
      "训练轮数： 5000 ，准确率： 98.1800019741 %\n",
      "训练轮数： 6000 ，准确率： 98.2400000095 %\n",
      "训练轮数： 7000 ，准确率： 98.2200026512 %\n",
      "训练轮数： 8000 ，准确率： 98.1999993324 %\n",
      "训练轮数： 9000 ，准确率： 98.2599973679 %\n",
      "训练轮数： 10000 ，准确率： 98.2400000095 %\n",
      "训练轮数： 11000 ，准确率： 98.2400000095 %\n",
      "训练轮数： 12000 ，准确率： 98.1599986553 %\n",
      "训练轮数： 13000 ，准确率： 98.2599973679 %\n",
      "训练轮数： 14000 ，准确率： 98.299998045 %\n",
      "训练轮数： 15000 ，准确率： 98.4200000763 %\n",
      "训练轮数： 16000 ，准确率： 98.2800006866 %\n",
      "训练轮数： 17000 ，准确率： 98.3799993992 %\n",
      "训练轮数： 18000 ，准确率： 98.3600020409 %\n",
      "训练轮数： 19000 ，准确率： 98.3200013638 %\n",
      "训练轮数： 20000 ，准确率： 98.3399987221 %\n",
      "训练轮数： 21000 ，准确率： 98.3799993992 %\n",
      "训练轮数： 22000 ，准确率： 98.400002718 %\n",
      "训练轮数： 23000 ，准确率： 98.400002718 %\n",
      "训练轮数： 24000 ，准确率： 98.4200000763 %\n",
      "训练轮数： 25000 ，准确率： 98.3200013638 %\n",
      "训练轮数： 26000 ，准确率： 98.4200000763 %\n",
      "训练轮数： 27000 ，准确率： 98.3799993992 %\n",
      "训练轮数： 28000 ，准确率： 98.400002718 %\n",
      "训练轮数： 29000 ，准确率： 98.3200013638 %\n",
      "训练轮数： 30000 ，准确率： 98.3900010586 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# MNIST数据集相关常数，其中输入节点数为每一张图片 28 * 28 的像素数，输出的节点数就是分类的个数 10； LAYER1_NODE 为隐藏层节点数，\n",
    "# BATCH_SIZE 为每次训练数据的个数；LEARNING_RATE_BASE 为基础学习率，LEARNING_RATE_DECAY 为学习率的衰减率，\n",
    "# REGULARIZATION_RATE 为正则化损失函数的系数，TRAINING_STEPS 为训练的次数，MOVING_AVERAGE_DECAY 为滑动平均衰减率\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 这个函数用来计算神经网络的向前传播结果，并且通过 RELU 函数实现了去线性化。avg_class 参数是用来支持测试时使用滑动平均模型。\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    if avg_class is None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "# 输入层\n",
    "x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "\n",
    "# 生成隐藏层参数\n",
    "weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "# 生成输出层参数\n",
    "weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "# 计算当前参数下神经网络向前传播的效果。\n",
    "y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "# 这个变量用来存储当前训练的次数。\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# 这里通过滑动平均衰减率和训练次数初始化这个类，用来加快训练早期变量的更新速度。\n",
    "variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "# 这里将所有的神经网络的上参数使用滑动平均，对于指定 trainable=False 的参数不作用。\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "# 计算使用了滑动平均模型处理的向前传播结果。\n",
    "average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "# 计算交叉熵，用来刻画预测值与真实值差距的损失函数，第一个参数是向前传播的结果，第二个是训练数据的答案。\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "# 计算所有样例的交叉熵平均值。\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# 计算 L2 正则化损失函数\n",
    "regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "# 计算模型的正则化损失，计算权重的，不计算偏置。\n",
    "regularization = regularizer(weights1) + regularizer(weights2)\n",
    "# 总损失等于交叉熵损失和正则化损失之和。\n",
    "loss = cross_entropy_mean + regularization\n",
    "# 设置指数衰减的学习率。\n",
    "learnging_rate = tf.train.exponential_decay(\n",
    "    LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "# 使用优化算法优化总损失。\n",
    "train_step = tf.train.GradientDescentOptimizer(learnging_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# 每过一次数据需要更新一下参数。\n",
    "with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "    train_op = tf.no_op()\n",
    "\n",
    "# 检验使用了滑动平均模型的向前传播结果是否正确。\n",
    "correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "# 计算平均准确率。\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化全部变量\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                        y_: mnist.validation.labels}\n",
    "        # 准备测试数据\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                # 使用全部的验证数据去做了验证\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print \"训练轮数：\", i, \"，准确率：\", validate_acc * 100, \"%\"\n",
    "            # 取出一部分训练数据\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            # 训练\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        # 计算最终的准确率。\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print \"训练轮数：\", TRAINING_STEPS, \"，准确率：\", test_acc * 100, \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> 下一节总结 准确率、交叉熵平均值、总损失、学习率、平均绝对梯度 的变化趋势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
